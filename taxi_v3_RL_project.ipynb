{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy\n",
    "import random\n",
    "from os import system, name\n",
    "from time import sleep\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the environment and the problem's parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Taxi-v3\").env "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_table = numpy.zeros([env.observation_space.n, env.action_space.n]) # instantiate q-table\n",
    "training_episodes = 20000 # number of training episodes\n",
    "test_episodes = 10 # number of test episodes\n",
    "display_episodes = 10 # number of episodes to display\n",
    "done = False # flag to indicate if episode is done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1 # learning rate\n",
    "gamma = 0.6 # discount factor\n",
    "epsilon = 0.1 # exploration - exploitation tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training of the agent using *Q-learning*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qlearning:\n",
    "    \n",
    "    def __init__(self, environment, gamma, alpha, epsilon, training_episodes, q_table, done):\n",
    "        self.environment = environment\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "        self.episodes = training_episodes\n",
    "        self.Q_table =  q_table\n",
    "\n",
    "    def apply(self):\n",
    "        for i in range(self.episodes):\n",
    "            state = self.environment.reset()[0] # Reset returns observation state and other info, but we only need the state number, which is in the first position of the tuple.\n",
    "            done = False # The episode is not done yet\n",
    "            penalties, reward = 0, 0 # Initialize penalties and reward to 0\n",
    "            \n",
    "            while not done:\n",
    "                if random.uniform(0, 1) < self.epsilon: # Check if agent should explore (random action) or exploit (choose the action that maximizes the reward).\n",
    "                    action = self.environment.action_space.sample() # Pick a possible new action for this state.\n",
    "                else:\n",
    "                    action = numpy.argmax(self.Q_table[state]) # Pick the action which has previously given the highest reward.\n",
    "                \n",
    "                next_state, reward, done, info, *_ = self.environment.step(action) \n",
    "                old_value = self.Q_table[state, action] # Retrieve old value from the Q-table.\n",
    "                next_max = numpy.max(self.Q_table[next_state])\n",
    "                temp_diff = next_max - old_value # Calculate the temporal difference.\n",
    "                # Update Q-value for current state.\n",
    "                new_value = old_value + self.alpha * (reward + self.gamma * temp_diff)\n",
    "                self.Q_table[state, action] = new_value\n",
    "                if reward == -10: # Check if agent attempted to do an illegal action.\n",
    "                    penalties += 1\n",
    "    \n",
    "                state = next_state\n",
    "                \n",
    "            if i % 100 == 0: # Output number of completed episodes every 100 episodes.\n",
    "                \n",
    "                print(f\"Episode: {i}\")\n",
    "    \n",
    "        print(\"Training finished.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_learning = Qlearning(env, gamma, alpha, epsilon, training_episodes, q_table, done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ambra\\anaconda3\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0\n",
      "Episode: 100\n",
      "Episode: 200\n",
      "Episode: 300\n",
      "Episode: 400\n",
      "Episode: 500\n",
      "Episode: 600\n",
      "Episode: 700\n",
      "Episode: 800\n",
      "Episode: 900\n",
      "Episode: 1000\n",
      "Episode: 1100\n",
      "Episode: 1200\n",
      "Episode: 1300\n",
      "Episode: 1400\n",
      "Episode: 1500\n",
      "Episode: 1600\n",
      "Episode: 1700\n",
      "Episode: 1800\n",
      "Episode: 1900\n",
      "Episode: 2000\n",
      "Episode: 2100\n",
      "Episode: 2200\n",
      "Episode: 2300\n",
      "Episode: 2400\n",
      "Episode: 2500\n",
      "Episode: 2600\n",
      "Episode: 2700\n",
      "Episode: 2800\n",
      "Episode: 2900\n",
      "Episode: 3000\n",
      "Episode: 3100\n",
      "Episode: 3200\n",
      "Episode: 3300\n",
      "Episode: 3400\n",
      "Episode: 3500\n",
      "Episode: 3600\n",
      "Episode: 3700\n",
      "Episode: 3800\n",
      "Episode: 3900\n",
      "Episode: 4000\n",
      "Episode: 4100\n",
      "Episode: 4200\n",
      "Episode: 4300\n",
      "Episode: 4400\n",
      "Episode: 4500\n",
      "Episode: 4600\n",
      "Episode: 4700\n",
      "Episode: 4800\n",
      "Episode: 4900\n",
      "Episode: 5000\n",
      "Episode: 5100\n",
      "Episode: 5200\n",
      "Episode: 5300\n",
      "Episode: 5400\n",
      "Episode: 5500\n",
      "Episode: 5600\n",
      "Episode: 5700\n",
      "Episode: 5800\n",
      "Episode: 5900\n",
      "Episode: 6000\n",
      "Episode: 6100\n",
      "Episode: 6200\n",
      "Episode: 6300\n",
      "Episode: 6400\n",
      "Episode: 6500\n",
      "Episode: 6600\n",
      "Episode: 6700\n",
      "Episode: 6800\n",
      "Episode: 6900\n",
      "Episode: 7000\n",
      "Episode: 7100\n",
      "Episode: 7200\n",
      "Episode: 7300\n",
      "Episode: 7400\n",
      "Episode: 7500\n",
      "Episode: 7600\n",
      "Episode: 7700\n",
      "Episode: 7800\n",
      "Episode: 7900\n",
      "Episode: 8000\n",
      "Episode: 8100\n",
      "Episode: 8200\n",
      "Episode: 8300\n",
      "Episode: 8400\n",
      "Episode: 8500\n",
      "Episode: 8600\n",
      "Episode: 8700\n",
      "Episode: 8800\n",
      "Episode: 8900\n",
      "Episode: 9000\n",
      "Episode: 9100\n",
      "Episode: 9200\n",
      "Episode: 9300\n",
      "Episode: 9400\n",
      "Episode: 9500\n",
      "Episode: 9600\n",
      "Episode: 9700\n",
      "Episode: 9800\n",
      "Episode: 9900\n",
      "Episode: 10000\n",
      "Episode: 10100\n",
      "Episode: 10200\n",
      "Episode: 10300\n",
      "Episode: 10400\n",
      "Episode: 10500\n",
      "Episode: 10600\n",
      "Episode: 10700\n",
      "Episode: 10800\n",
      "Episode: 10900\n",
      "Episode: 11000\n",
      "Episode: 11100\n",
      "Episode: 11200\n",
      "Episode: 11300\n",
      "Episode: 11400\n",
      "Episode: 11500\n",
      "Episode: 11600\n",
      "Episode: 11700\n",
      "Episode: 11800\n",
      "Episode: 11900\n",
      "Episode: 12000\n",
      "Episode: 12100\n",
      "Episode: 12200\n",
      "Episode: 12300\n",
      "Episode: 12400\n",
      "Episode: 12500\n",
      "Episode: 12600\n",
      "Episode: 12700\n",
      "Episode: 12800\n",
      "Episode: 12900\n",
      "Episode: 13000\n",
      "Episode: 13100\n",
      "Episode: 13200\n",
      "Episode: 13300\n",
      "Episode: 13400\n",
      "Episode: 13500\n",
      "Episode: 13600\n",
      "Episode: 13700\n",
      "Episode: 13800\n",
      "Episode: 13900\n",
      "Episode: 14000\n",
      "Episode: 14100\n",
      "Episode: 14200\n",
      "Episode: 14300\n",
      "Episode: 14400\n",
      "Episode: 14500\n",
      "Episode: 14600\n",
      "Episode: 14700\n",
      "Episode: 14800\n",
      "Episode: 14900\n",
      "Episode: 15000\n",
      "Episode: 15100\n",
      "Episode: 15200\n",
      "Episode: 15300\n",
      "Episode: 15400\n",
      "Episode: 15500\n",
      "Episode: 15600\n",
      "Episode: 15700\n",
      "Episode: 15800\n",
      "Episode: 15900\n",
      "Episode: 16000\n",
      "Episode: 16100\n",
      "Episode: 16200\n",
      "Episode: 16300\n",
      "Episode: 16400\n",
      "Episode: 16500\n",
      "Episode: 16600\n",
      "Episode: 16700\n",
      "Episode: 16800\n",
      "Episode: 16900\n",
      "Episode: 17000\n",
      "Episode: 17100\n",
      "Episode: 17200\n",
      "Episode: 17300\n",
      "Episode: 17400\n",
      "Episode: 17500\n",
      "Episode: 17600\n",
      "Episode: 17700\n",
      "Episode: 17800\n",
      "Episode: 17900\n",
      "Episode: 18000\n",
      "Episode: 18100\n",
      "Episode: 18200\n",
      "Episode: 18300\n",
      "Episode: 18400\n",
      "Episode: 18500\n",
      "Episode: 18600\n",
      "Episode: 18700\n",
      "Episode: 18800\n",
      "Episode: 18900\n",
      "Episode: 19000\n",
      "Episode: 19100\n",
      "Episode: 19200\n",
      "Episode: 19300\n",
      "Episode: 19400\n",
      "Episode: 19500\n",
      "Episode: 19600\n",
      "Episode: 19700\n",
      "Episode: 19800\n",
      "Episode: 19900\n",
      "Training finished.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q_learning.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent's evaluation after learning with *Q-learning*:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the following code cell, a window will open, in order to wisualize what the trained agent is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "EPISODE  1 \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ambra\\anaconda3\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You reached your destination!\n",
      "\n",
      "\n",
      "EPISODE  2 \n",
      "\n",
      "\n",
      "You reached your destination!\n",
      "\n",
      "\n",
      "EPISODE  3 \n",
      "\n",
      "\n",
      "You reached your destination!\n",
      "\n",
      "\n",
      "EPISODE  4 \n",
      "\n",
      "\n",
      "You reached your destination!\n",
      "\n",
      "\n",
      "EPISODE  5 \n",
      "\n",
      "\n",
      "You reached your destination!\n",
      "\n",
      "\n",
      "EPISODE  6 \n",
      "\n",
      "\n",
      "You reached your destination!\n",
      "\n",
      "\n",
      "EPISODE  7 \n",
      "\n",
      "\n",
      "You reached your destination!\n",
      "\n",
      "\n",
      "EPISODE  8 \n",
      "\n",
      "\n",
      "You reached your destination!\n",
      "\n",
      "\n",
      "EPISODE  9 \n",
      "\n",
      "\n",
      "You reached your destination!\n",
      "\n",
      "\n",
      "EPISODE  10 \n",
      "\n",
      "\n",
      "You reached your destination!\n"
     ]
    }
   ],
   "source": [
    "for episode in range(test_episodes):\n",
    "    env = gym.make(\"Taxi-v3\", render_mode=\"human\").env\n",
    "    state = env.reset()[0]\n",
    "    done = False\n",
    "    print(\"\\n\\nEPISODE \", episode+1, \"\\n\\n\")\n",
    "    time.sleep(1)\n",
    "\n",
    "    for step in range(0,20):        \n",
    "        env.render()\n",
    "        time.sleep(0.3)\n",
    "\n",
    "        action = numpy.argmax(q_table[state])        \n",
    "        new_state, reward, done, info, *_ = env.step(action)\n",
    "        \n",
    "        if done:\n",
    "            env.render()\n",
    "            if reward == 20:\n",
    "                print(\"You reached your destination!\")\n",
    "                time.sleep(1.5)\n",
    "            break\n",
    "\n",
    "        state = new_state\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f8710ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamic programming for stochastic planning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0c43dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "import random\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8521d9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python way to say that BaseEnvironment is an abstract class:\n",
    "# BaseEnvironment inherits from ABC\n",
    "class BaseEnvironment(ABC):\n",
    "    # Python way to describe an abstract method\n",
    "    # Override thim method to return the set of states\n",
    "    @abstractmethod\n",
    "    def get_states(self):\n",
    "        pass\n",
    "    \n",
    "    # Override this method to return the set of actions from a state\n",
    "    @abstractmethod\n",
    "    def get_actions(self):\n",
    "        pass\n",
    "    \n",
    "    # Override this method to return all the actions\n",
    "    @abstractmethod\n",
    "    def get_all_actions(self):\n",
    "        pass\n",
    "    \n",
    "    # Override this method to implement p(s'|s, a)\n",
    "    # The environment is meant to be used for stochastic planning\n",
    "    # so I need to know the state transition probabilities\n",
    "    @abstractmethod\n",
    "    def p(self,surrent_state, action, next_state):\n",
    "        pass\n",
    "    \n",
    "    # Override this method to implement r(s, a, s')\n",
    "    @abstractmethod\n",
    "    def r(self, current_state, action, next_state):\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2836d5a4",
   "metadata": {},
   "source": [
    "Let's introduce a concrete Environment implementing a GridWorld:\n",
    "<ul>\n",
    "    <li> 4x4 grid </li>\n",
    "    <li> the agent can move up and down, left and right (no diagonal moves)\n",
    "</li>\n",
    "    <li> transitions are deterministic </li>\n",
    "    <li> cell(3,3) is the goal </li>\n",
    "    <li> reward is -1 for moving and 10 for reaching the target </li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3cf72462",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld(BaseEnvironment):\n",
    "    \n",
    "    # Define actions as pairs implementing movements\n",
    "    UP = (-1,0) # going up means you subtract one from the current row\n",
    "    DOWN = (1,0)\n",
    "    RIGHT = (0, 1) # going right means you add one to the current column\n",
    "    LEFT = (0,-1)\n",
    "    \n",
    "    # Define the startign position and the goal\n",
    "    START=(0,0)\n",
    "    GOAL=(3,3)\n",
    "    \n",
    "    # A dictionary giving labels to actions\n",
    "    label = {(-1,0) : \"UP\", (1,0) : \"DOWN\", (0,1) : \"RIGHT\", (0,-1) : \"LEFT\"}\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.states = list()\n",
    "        # It is a bad coding practice to put \"magic constants\" in code\n",
    "        # We create the set of states as a list of pairs:\n",
    "        # (0,0) (0,1), ..., (1,0), (1,1), ..., (3,2), (3,3)\n",
    "        for ro in range(4):\n",
    "            for col in range(4):\n",
    "                self.states.append((ro,col))\n",
    "                \n",
    "        # The set of actions is a dictionary state-actions\n",
    "        self.actions = dict()\n",
    "        # I wxplicitly list the mapping\n",
    "        self.actions[(0,0)] = [GridWorld.RIGHT, GridWorld.DOWN]\n",
    "        self.actions[(0,1)] = [GridWorld.RIGHT, GridWorld.LEFT, GridWorld.DOWN]\n",
    "        self.actions[(0,2)] = [GridWorld.RIGHT, GridWorld.LEFT, GridWorld.DOWN]\n",
    "        self.actions[(0,3)] = [GridWorld.LEFT, GridWorld.DOWN]\n",
    "        self.actions[(1,0)] = [GridWorld.RIGHT, GridWorld.DOWN, GridWorld.UP]\n",
    "        self.actions[(1,1)] = [GridWorld.RIGHT, GridWorld.LEFT, GridWorld.DOWN, GridWorld.UP]\n",
    "        self.actions[(1,2)] = [GridWorld.RIGHT, GridWorld.DOWN, GridWorld.UP]\n",
    "        self.actions[(1,3)] = [GridWorld.LEFT, GridWorld.DOWN, GridWorld.UP]\n",
    "        self.actions[(2,0)] = [GridWorld.RIGHT, GridWorld.DOWN, GridWorld.UP]\n",
    "        self.actions[(2,1)] = [GridWorld.RIGHT, GridWorld.LEFT, GridWorld.DOWN, GridWorld.UP]\n",
    "        self.actions[(2,2)] = [GridWorld.RIGHT, GridWorld.LEFT,GridWorld.DOWN, GridWorld.UP]\n",
    "        self.actions[(2,3)] = [GridWorld.LEFT, GridWorld.DOWN, GridWorld.UP]\n",
    "        self.actions[(3,0)] = [GridWorld.RIGHT, GridWorld.UP]\n",
    "        self.actions[(3,1)] = [GridWorld.RIGHT, GridWorld.LEFT, GridWorld.UP]\n",
    "        self.actions[(3,2)] = [GridWorld.RIGHT, GridWorld.LEFT, GridWorld.UP]\n",
    "        self.actions[(3,3)] = [GridWorld.LEFT, GridWorld.UP, GridWorld.GOAL]\n",
    "        self.all_actions = [GridWorld.START, GridWorld.LEFT, GridWorld.RIGHT, GridWorld.DOWN, GridWorld.UP]\n",
    "    \n",
    "    # Implementation of abstract method get_states\n",
    "    def get_states(self):\n",
    "        return self.states\n",
    "    \n",
    "        # Implementation of abstract method get_actions\n",
    "    def get_actions(self, state):\n",
    "        return self.actions[state]\n",
    "    \n",
    "        # Implementation of abstract method get_all_actions\n",
    "    def get_all_actions(self):\n",
    "        return self.all_actions\n",
    "    \n",
    "    # Introducing an \"helper\" method to compute a transition:\n",
    "    # returns the next state given the current one and an action\n",
    "    # E.g., if current state is (0,0) and action is (0,1) (action RIGHT) \n",
    "    # the next state will be (0+0, 0+1) = (0,1)\n",
    "    def transition(self, current_state, action):\n",
    "        return (current_state[0]+action[0], current_state[1]+action[1])\n",
    "    \n",
    "    # Implementation of p\n",
    "    def p(self, current_state, action, next_state):\n",
    "        # If the action is not executable p(s,a,s')=0\n",
    "        if action not in self.get_actions(current_state):\n",
    "            return 0\n",
    "        # Which state do I get if I execute the action?\n",
    "        new_state = self.transition(current_state, action)\n",
    "        # The problem is deterministic\n",
    "        if (new_state==next_state):\n",
    "            return 1\n",
    "        else: \n",
    "            return 0\n",
    "        \n",
    "        # Implementation of r\n",
    "    def r(self, current_state, action, next_state):\n",
    "        # Check whether next_state is reachable from the current one\n",
    "        if (self.p(current_state, action, next_state) > 0):\n",
    "            if (next_state != GridWorld.GOAL):\n",
    "                # The action is executable and the next state is not the GOAL\n",
    "                return -1\n",
    "            else:\n",
    "                # the action is executable and the next state is the GOAL\n",
    "                return 10\n",
    "        else:\n",
    "            # The action is not executable: the robot woll not attempt this, \n",
    "            # but is added for sake of completeness\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0143a1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3073e59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0), (0, -1), (0, 1), (1, 0), (-1, 0)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gw = GridWorld()\n",
    "gw.get_all_actions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02a4d71e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0), (0, -1), (0, 1), (1, 0), (-1, 0)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gw.get_all_actions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e39d1847",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1), (1, 0)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gw.get_actions((0,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d801c03e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What is the probability of ending in (0,1) if in (0,0) and going RIGHT?\n",
    "gw.p((0,0), GridWorld.RIGHT, (0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f22ff52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#what is the roward for taht?\n",
    "gw.r((0,0),GridWorld.RIGHT, (0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a65af48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#introduce some code to handle policies : a policy can be either applied as it is or updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd58163b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasePolicy(ABC):\n",
    "    # returns the action to apply in the state\n",
    "    \n",
    "    @abstractmethod\n",
    "    def apply(self, state):\n",
    "        pass\n",
    "    # change the action to apply in the state\n",
    "    @abstractmethod\n",
    "    def update(self,state,action):\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a477f186",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomPolicy(BasePolicy):\n",
    "    #environment should be an instance of BaseEnvironment\n",
    "    def __init__(self, environment):\n",
    "        #keep trace of the environment internally\n",
    "        self.environment=environment\n",
    "        #map states to actions: initially no mapping\n",
    "        self.state_action_table=dict()\n",
    "        for states in environment.get_states():\n",
    "            self.state_action_table[states]= None\n",
    "    def apply(self,state):\n",
    "        if self.state_action_table[state]==None:\n",
    "        #if no action is specified for that state, choose at random\n",
    "            actions=self.environment.get_actions(state)\n",
    "            return random.choice(actions)\n",
    "    #if an action is available, rteturtn that one\n",
    "        return self.state_action_table[state]\n",
    "    def update(self, state, action):\n",
    "        self.state_action_table[state]= action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3180ee1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's custom policy\n",
    "policy=CustomPolicy(gw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1237a447",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DOWN'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action=policy.apply((0,0))\n",
    "GridWorld.label[action]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba2581a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyIteration:\n",
    "    \n",
    "    #gamma is the discount factor\n",
    "    # theta is the threshold for the convergence in policy evaluation\n",
    "    def __init__(self,environment, gamma, theta):\n",
    "        self.environment = environment\n",
    "        self.gamma = gamma\n",
    "        self.theta = theta\n",
    "        #store the current policy (in the end should be the optimal one)\n",
    "        self.policy = CustomPolicy(environment)\n",
    "        #store the value of a state a k.th step\n",
    "        self.old_value = dict()\n",
    "        #store the value of a state at k+1.th step\n",
    "        self.new_value = dict()\n",
    "        # initialize the value of each state to 0\n",
    "        for state in environment.get_states():\n",
    "            self.old_value[state] = 0\n",
    "\n",
    "    def copy_values(self, from_value, to_value):\n",
    "        for state in self.environment.get_states():\n",
    "            to_value[state]=from_value[state]\n",
    "\n",
    "    def evaluate_policy(self):\n",
    "        #repeat until delta<theta (slides)\n",
    "        while(True):\n",
    "            delta=0\n",
    "            for state in self.environment.get_states():\n",
    "                #the policy is deterministic!!\n",
    "                #otherwise need another for loop for all the possible policy actions\n",
    "                action=self.policy.apply(state)\n",
    "                self.new_value[state]=0\n",
    "                #check all potential next states\n",
    "                for next_state in self.environment.get_states():\n",
    "                    self.new_value[state]+=self.environment.p(state, action, next_state)*\\\n",
    "                    (self.environment.r(state,action,next_state)+self.gamma*self.old_value[next_state])\n",
    "                delta=max(delta,abs(self.old_value[state]-self.new_value[state]))\n",
    "            #copy the values into old values\n",
    "            self.copy_values(self.new_value,self.old_value)\n",
    "            if delta<self.theta:\n",
    "                return\n",
    "    \n",
    "    #return true if policy is improved and false otherwise\n",
    "    def policy_improvement(self):\n",
    "        policy_improved=False\n",
    "        for state in self.environment.get_states():\n",
    "            old_action=self.policy.apply(state)\n",
    "            #old action is the action suggested by the current policy\n",
    "            #we must compute the action yielding the max return from the state\n",
    "            max_value_of_action=-1*sys.float_info.max #smallest float\n",
    "            best_action=old_action\n",
    "            for action in self.environment.get_actions(state):\n",
    "                value_of_action=0\n",
    "            for next_state in self.environment.get_states():\n",
    "                value_of_action+=self.environment.p(state, action, next_state)*\\\n",
    "                    (self.environment.r(state,action,next_state)+self.gamma*self.old_value[next_state])\n",
    "            if value_of_action>max_value_of_action:\n",
    "                max_value_of_action=value_of_action\n",
    "                best_action=action\n",
    "        #when i'm done checking, i know the best\n",
    "        if old_action!=best_action:\n",
    "            self.policy.update(state,best_action)\n",
    "            policy_improved=True\n",
    "        return policy_improved\n",
    "    #compute policy iteration\n",
    "    def apply(self):\n",
    "        while True:\n",
    "            self.evaluate_policy()\n",
    "            if not self.policy_improvement():\n",
    "                return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "48cfef75",
   "metadata": {},
   "outputs": [],
   "source": [
    "gw=GridWorld()\n",
    "#initialize policy iteration function object with gamma=0.8 and delta=10^-5\n",
    "policy_iteration=PolicyIteration(gw,0.8,1/100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d20d5fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_iteration.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b9a1a84c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-4.999999951148707 -4.999999999999973 -4.999999951148707 -4.999999999999973 \n",
      "-4.999999999999973 -4.999999951148707 -4.999999999999973 -4.999999951148707 \n",
      "-4.999999951148707 -4.999999999999973 -4.999999951148707 -4.999999999999973 \n",
      "-4.999999999999973 -4.999999951148707 -4.999999999999973 0.0 \n"
     ]
    }
   ],
   "source": [
    "#print out the value function (should be the optimal one)\n",
    "for ro in range(4):\n",
    "    for co in range(4):\n",
    "        print(policy_iteration.old_value[(ro,co)],end=' ')\n",
    "    print()\n",
    "    \n",
    "#controlla gli errori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0f610dc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RIGHT DOWN LEFT DOWN \n",
      "RIGHT LEFT UP UP \n",
      "RIGHT UP DOWN DOWN \n",
      "UP LEFT LEFT LEFT \n"
     ]
    }
   ],
   "source": [
    "#print out the value function (should be the optimal one)\n",
    "for ro in range(4):\n",
    "    for co in range(4):\n",
    "        print(GridWorld.label[policy_iteration.policy.apply((ro,co))],end=' ')\n",
    "    print()\n",
    "    #????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fc276328",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SlipperyGridWorld(GridWorld):\n",
    "    \n",
    "    #gamma is the discount factor\n",
    "    # theta is the threshold for the convergence in policy evaluation\n",
    "    def __init__(self):\n",
    "        super(SlipperyGridWorld, self).__init__()\n",
    "    def p(self, current_state,action,next_state):\n",
    "        if action not in self.get_actions(current_state):\n",
    "            return 0\n",
    "        new_state=self.transition(current_state,action)\n",
    "        if (new_state==next_state):\n",
    "            return 0.9\n",
    "        elif (new_state==current_state):\n",
    "            return 0.1\n",
    "        else:\n",
    "            return 0\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "67403f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgw=SlipperyGridWorld()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f8047d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pi=PolicyIteration(sgw,0.8,1/100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3df24c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pi.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0216b8fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.214285714285695 -3.214283981168631 -3.214285714285695 -3.214283981168631 \n",
      "-3.214283981168631 -3.214285714285695 -3.214283981168631 -3.214285714285695 \n",
      "-3.214285714285695 -3.214283981168631 -3.214285714285695 -3.214283981168631 \n",
      "-3.214283981168631 -3.214285714285695 -3.214283981168631 0.0 \n"
     ]
    }
   ],
   "source": [
    "for ro in range(4):\n",
    "    for co in range(4):\n",
    "        print(pi.old_value[(ro,co)],end=' ')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "69babf21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOWN RIGHT DOWN LEFT \n",
      "UP UP UP LEFT \n",
      "UP RIGHT UP LEFT \n",
      "UP UP UP "
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "(3, 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ro \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m4\u001b[39m):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m co \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m4\u001b[39m):\n\u001b[1;32m----> 3\u001b[0m         \u001b[38;5;28mprint\u001b[39m(GridWorld\u001b[38;5;241m.\u001b[39mlabel[pi\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39mapply((ro,co))],end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m()\n",
      "\u001b[1;31mKeyError\u001b[0m: (3, 3)"
     ]
    }
   ],
   "source": [
    "for ro in range(4):\n",
    "    for co in range(4):\n",
    "        print(GridWorld.label[pi.policy.apply((ro,co))],end=' ')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cbd35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VALUE ITERATION ALGORITHM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0caf47f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#base attributes and the copy function could be factored out in a base class for both policies\n",
    "class ValueIteration:\n",
    "    \n",
    "    #gamma is the discount factor\n",
    "    # theta is the threshold for the convergence in policy evaluation\n",
    "    def __init__(self,environment, gamma, theta):\n",
    "        self.environment = environment\n",
    "        self.gamma = gamma\n",
    "        self.theta = theta\n",
    "        #store the current policy (in the end should be the optimal one)\n",
    "        self.policy = CustomPolicy(environment)\n",
    "        #store the value of a state a k.th step\n",
    "        self.old_value = dict()\n",
    "        #store the value of a state at k+1.th step\n",
    "        self.new_value = dict()\n",
    "        # initialize the value of each state to 0\n",
    "        for state in environment.get_states():\n",
    "            self.old_value[state] = 0\n",
    "\n",
    "    def copy_values(self, from_value, to_value):\n",
    "        for state in self.environment.get_states():\n",
    "            to_value[state]=from_value[state]\n",
    "\n",
    "    def compute_values(self):\n",
    "        #repeat until delta<theta (slides)\n",
    "        while(True):\n",
    "            delta=0\n",
    "            for state in self.environment.get_states():\n",
    "                #compute new value of the state\n",
    "                max_value=-1*sys.float_info.max #smallest float\n",
    "                for action in self.environment.get_actions(state):\n",
    "                    value=0\n",
    "                for next_state in self.environment.get_states():\n",
    "                    value+=self.environment.p(state, action, next_state)*\\\n",
    "                        (self.environment.r(state,action,next_state)+self.gamma*self.old_value[next_state])\n",
    "                if value>max_value:\n",
    "                    max_value=value\n",
    "                    \n",
    "                #the policy is deterministic!!\n",
    "                #otherwise need another for loop for all the possible policy actions\n",
    "                \n",
    "                self.new_value[state]=max_value\n",
    "                delta=max(delta,abs(self.old_value[state]-self.new_value[state]))\n",
    "            #copy the values into old values\n",
    "            self.copy_values(self.new_value,self.old_value)\n",
    "            if delta<self.theta:\n",
    "                return\n",
    "    \n",
    "    #compute policy\n",
    "    def compute_policy(self):\n",
    "        for state in self.environment.get_states():\n",
    "            # we must compute the action yielding the max return state\n",
    "            max_value_of_action=-1*sys.float_info.max #smallest float\n",
    "            best_action=None\n",
    "            for action in self.environment.get_actions(state):\n",
    "                value_of_action=0\n",
    "            for next_state in self.environment.get_states():\n",
    "                value_of_action+=self.environment.p(state, action, next_state)*\\\n",
    "                    (self.environment.r(state,action,next_state)+self.gamma*self.old_value[next_state])\n",
    "            if value_of_action>max_value_of_action:\n",
    "                max_value_of_action=value_of_action\n",
    "                best_action=action\n",
    "        #when i'm done checking, i know the best\n",
    "        self.policy.update(state,best_action)\n",
    "        \n",
    "    #compute policy according to value iteration\n",
    "    def apply(self):\n",
    "        self.compute_values()\n",
    "        self.compute_policy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14455794",
   "metadata": {},
   "outputs": [],
   "source": [
    "vi=ValueIteration(gw,0.8,1/100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafc59b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vi.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1ebd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ro in range(4):\n",
    "    for co in range(4):\n",
    "        print(GridWorld.label[vi.policy.apply((ro,co))],end=' ')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83069948",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis=ValueIteration(sgw,0.8,1/100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcd567a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a02476",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
